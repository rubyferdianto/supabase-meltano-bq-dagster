# S3-RDS-BQ-Airflow Project

This project demonstrates a comprehensive data pipeline with multiple import workflows into AWS RDS MySQL database.

## ğŸ—‚ï¸ Project Structure

```
s3-rds-bq-airflow/
â”œâ”€â”€ main.py                       # ğŸ¯ Main orchestrator (runs all components)
â”œâ”€â”€ CSV-RDS/                     # ğŸ“ RDS CSV import workflows
â”‚   â”œâ”€â”€ s3-to-rds.py             # ğŸ“¥ S3 to RDS import workflow (pandas-based)
â”‚   â”œâ”€â”€ csv-to-rds-via-s3.py     # ğŸ“¥ Local CSV to RDS import workflow
â”‚   â”œâ”€â”€ create-rds-instance.py   # ğŸ”§ Automated RDS instance creation
â”‚   â””â”€â”€ backup/                  # ğŸ“ Backup of old scripts
â”œâ”€â”€ csv-to-rds/                  # ğŸ“¥ Local CSV staging folder
â”œâ”€â”€ csv-imported-to-rds/         # ğŸ“ Local CSV completed folder
â”œâ”€â”€ RDS-BQ/                      # ğŸ“ RDS to BigQuery workflows
â”œâ”€â”€ .env                         # ğŸ” Database credentials (gitignored)
â”œâ”€â”€ .env.example                 # ğŸ“‹ Template for environment variables
â”œâ”€â”€ requirements-bec.yaml        # ğŸ Conda environment specification
â””â”€â”€ README.md                    # ğŸ“– This file
```

## ğŸš€ Quick Start

### 1. Environment Setup
```bash
# Create conda environment
conda env create -f requirements-bec.yaml
conda activate bec
```

### 2. Database Configuration
```bash
# Copy and edit environment variables
cp .env.example .env
# Edit .env with your actual AWS RDS MySQL credentials
```

### 3. Create RDS Instance (if needed)
```bash
# Create RDS MySQL instance with VPC and security groups
python CSV-RDS/create-rds-instance.py
```

### 4. Run Complete Pipeline
```bash
# Run the main orchestrator (recommended)
python main.py

# This will execute:
# 0. Database connectivity check
# 1. S3 to RDS import (from s3://bec-bucket-aws/s3-to-rds/)
# 2. Local CSV to RDS import (from ./csv-to-rds/)
```

### 5. Individual Components
```bash
# Database check only
python main.py  # (includes connectivity check)

# S3 to RDS import only
python CSV-RDS/s3-to-rds.py

# Local CSV to RDS import only
python CSV-RDS/csv-to-rds-via-s3.py
```

## ğŸ“Š Database Status

Successfully loaded **9 tables** with **451,322+ total rows**:

| Table | Rows | Description |
|-------|------|-------------|
| olist_customers_dataset | 99,441 | Customer information |
| olist_geolocation_dataset | 1,000,163 | Geographic data |
| olist_sellers_dataset | 3,095 | Seller information |
| olist_orders_dataset | 99,441 | Order details |
| olist_order_items_dataset | 112,650 | Order line items |
| olist_order_payments_dataset | 103,886 | Payment information |
| olist_order_reviews_dataset | 99,224 | Customer reviews |
| olist_products_dataset | 32,951 | Product catalog |
| product_category_name_translation | 71 | Category translations |

## ğŸ”„ **Workflow Options**

### Option 1: Complete Pipeline (Recommended)
```bash
python main.py
```
- **Step 0**: Database connectivity check  
- **Step 1**: S3 import â†’ RDS (pandas-based import)
- **Step 2**: Local CSV import â†’ RDS

### Option 2: S3 Import Workflow
1. **ğŸ“¥ Upload Files**: Place CSV files in S3 bucket `s3://bec-bucket-aws/s3-to-rds/`
2. **â–¶ï¸ Run Import**: Execute `python CSV-RDS/s3-to-rds.py`
3. **âœ… Auto Processing**: Script imports to RDS using pandas and moves files to `s3-imported-to-rds/`

### Option 3: Local CSV Import Workflow
1. **ğŸ“¥ Stage Files**: Place CSV files in `csv-to-rds/` folder
2. **â–¶ï¸ Run Import**: Execute `python CSV-RDS/csv-to-rds-via-s3.py`
3. **âœ… Auto Processing**: Script imports to RDS and moves files to `csv-imported-to-rds/`

## ğŸ› ï¸ Available Scripts

### Core Pipeline Components:
- **`main.py`** - Main orchestrator (runs all components in sequence)
- **`CSV-RDS/create-rds-instance.py`** - Automated RDS instance creation with VPC setup
- **`CSV-RDS/s3-to-rds.py`** - S3 to RDS import workflow with pandas-based processing
- **`CSV-RDS/csv-to-rds-via-s3.py`** - Local CSV to RDS import workflow

### Utility Scripts:
- **`show-storage.py`** - Show database storage details

## âœ¨ Key Features

- âœ… **Automated Pipeline** - Complete end-to-end workflow with one command
- âœ… **Dual Import Methods** - Both S3 and local CSV import capabilities
- âœ… **File Management** - Automatic file organization post-import
- âœ… **RDS Infrastructure** - Automated RDS instance creation with VPC
- âœ… **Error Handling** - Robust error management and logging
- âœ… **Progress Tracking** - Real-time import progress
- âœ… **MySQL Compatibility** - Optimized for AWS RDS MySQL
- âœ… **Cost Effective** - Uses standard RDS instead of Aurora for cost optimization
- âœ… **Pandas Integration** - Reliable pandas-based data processing

## ğŸ“ˆ Original Project Plan

1. Python will check andy CSV into S3 under bucket "BEC-BUCKET-AWS\S3-TO-RDS"
2. Python will also check if any CSV files ready to imported in local folder "CSV-TO-RDS"
2. Meltano will get from AWS S3 to AWS RDS
3. AWS RDS will transfer into GCP BigQuery
4. BigQuery process data analytic for factsales dimension
5. Visualisation get the factsales
6. Airflow will be used to monitor the process from steps 1 to 5

## ğŸ”§ Current Implementation Status

âœ… **Completed**: 
- Step 1: CSV â†’ S3 and direct CSV â†’ RDS loading
- Step 2: S3 â†’ RDS import workflow

â³ **Next**: Steps 3-6 (BigQuery, Analytics, Visualization, Airflow)

## ğŸ”§ Troubleshooting

- **Connection Issues**: Run `python main.py` to check database connectivity
- **Missing Tables**: Re-run the appropriate import script
- **Environment Issues**: Recreate conda environment with `conda env create -f requirements-bec.yaml`
- **Credentials**: Verify `.env` file configuration with RDS_* variables

## ğŸ’° Cost Optimization

This project has been optimized for cost by switching from Aurora to standard RDS MySQL:
- **Aurora**: More expensive but offers advanced features like S3 integration
- **RDS MySQL**: Cost-effective alternative using pandas-based processing
- **Free Tier**: Uses db.t3.micro instances eligible for AWS free tier

---
*Project Status: Successfully implemented dual CSV import workflows to AWS RDS MySQL with automated pipeline orchestration and cost optimization*
