# Supabase-Meltano-BigQuery Data```

A comprehensive end-to-end data pipeline that extracts data from Supabase PostgreSQL, loads it to Google BigQuery, and transforms it using dbt for analytics. Built with **Meltano ELT framework** for production-ready deployment and **Dagster** for orchestration.

**Note**: "bec" stands for "brazilian e-commerce" throughout the project naming convention.

## ğŸ¯ Complete Data Flow

```
Supabase PostgreSQL â†’ Google BigQuery â†’ dbt Transformations
        â†“                    â†“              â†“
   ï¿½ Raw Data         ğŸ­ Staging DB    ï¿½ Analytics
        â”‚                    â”‚              â”‚
        â””â”€â”€ Meltano ELT â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Requirements

### Python Version

- **Python 3.11** (Required for Meltano compatibility)
- **Not compatible with Python 3.13+** (dependency conflicts)

### Recommended Setup

- **Production**: Meltano ELT framework (default)
- **Development**: Direct Python approach (fallback)
- **Orchestration**: Dagster for workflow management
- **Transformation**: dbt for data modeling and analytics

## ğŸ—‚ï¸ Current Project Structure

```
supabase-meltano-bq-dagster/
â”œâ”€â”€ bec_dbt/                      # ğŸ“ dbt transformation layer
â”‚   â”œâ”€â”€ dbt_project.yml           # âš™ï¸ dbt project configuration
â”‚   â”œâ”€â”€ profiles.yml              # ï¿½ BigQuery connection profiles
â”‚   â”œâ”€â”€ models/                   # ğŸ“Š dbt models
â”‚   â”‚   â”œâ”€â”€ staging/              # ğŸ§¹ Raw data cleaning
â”‚   â”‚   â”œâ”€â”€ warehouse/            # ğŸ­ Dimensional modeling
â”‚   â”‚   â””â”€â”€ analytic/             # ï¿½ One Big Table analytics
â”‚   â”œâ”€â”€ macros/                   # ğŸ”§ Reusable SQL macros
â”‚   â””â”€â”€ README.md                 # ğŸ“– dbt documentation
â”œâ”€â”€ bec-dagster/                  # ğŸ¼ Orchestration framework
â”‚   â”œâ”€â”€ dagster_pipeline.py       # ğŸ¯ Dagster asset definitions
â”‚   â”œâ”€â”€ start_dagster.sh          # ğŸŒ Dagster web UI launcher
â”‚   â””â”€â”€ workspace.yaml            # âš™ï¸ Dagster configuration
â”œâ”€â”€ bec-meltano/                  # ğŸ“ Production Meltano ELT pipeline
â”‚   â”œâ”€â”€ meltano.yml               # âš™ï¸ Meltano configuration
â”‚   â”œâ”€â”€ rds-to-bq-meltano.py      # ï¿½ Meltano pipeline runner
â”‚   â”œâ”€â”€ delete-rds-after-load.py  # ğŸ§¹ RDS cleanup after transfer
â”‚   â”œâ”€â”€ meltano-post-hook.py      # ğŸ”— Post-transfer automation
â”‚   â”œâ”€â”€ plugins/                  # ğŸ”Œ Meltano extractors & loaders
â”‚   â”œâ”€â”€ .env.example              # ğŸ“‹ Environment template
â”‚   â”œâ”€â”€ .env                      # ğŸ” Environment configuration (gitignored)
â”‚   â””â”€â”€ .meltano/                 # ï¿½ Meltano state & metadata
â”œâ”€â”€ service-account-key.json # ğŸ”‘ Google Cloud service account key
â”œâ”€â”€ requirements-bec.yaml         # ğŸ Conda environment specification (ONLY requirements file)
â””â”€â”€ README.md                     # ğŸ“– This file
```

## ğŸš€ Quick Start - Complete Pipeline

### 1. Environment Setup

**Conda Environment (Recommended & Only Option)**

```bash
# Create conda environment with Python 3.11 and all dependencies
conda env create -f requirements-bec.yaml
conda activate bec

# Verify installation
python --version  # Should show 3.11.x
meltano --version # Should show 3.7.8+
```

### 2. Configure Environment Variables

```bash
# Copy Meltano environment template and edit with your credentials
cp bec-meltano/.env.example bec-meltano/.env
nano bec-meltano/.env  # Add your Supabase and BigQuery credentials
```

### 3. Add Google Service Account Key

Place your Google Cloud service account JSON key file in the project root:

```bash
# Copy your service account key to the root directory
cp /path/to/your/service-account-key.json ./service-account-key.json
```

**Note**: Both Meltano and dbt will read the key file path from the `GOOGLE_APPLICATION_CREDENTIALS` environment variable.

### 4. Run Complete Pipeline

```bash
# Use Dagster for complete orchestration (recommended)
cd bec-dagster/
./start_dagster.sh

# Or run Meltano ELT pipeline directly
cd bec-meltano/
meltano run supabase-to-bigquery-with-transform
```

## ğŸ¼ Orchestration & Execution Options

### Dagster (Recommended for Development & Monitoring)

```bash
# Start Dagster web UI
cd bec-dagster/
./start_dagster.sh

# Access web interface at http://127.0.0.1:3000
```

### Meltano ELT (Production)

```bash
# Production approach with Meltano
cd bec-meltano/

# Extract from Supabase to BigQuery with dbt transformations
meltano run supabase-to-bigquery-with-transform
```

### dbt Transformations (Data Modeling)

```bash
# Run dbt transformations
cd bec_dbt/

# Run all models
dbt run

# Run specific model layers
dbt run --models staging    # Raw data cleaning
dbt run --models warehouse  # Dimensional modeling
dbt run --models analytic   # Analytics aggregations

# Test data quality
dbt test
```

### Direct Python (Development/Testing)

```bash
# Direct approach for quick testing
cd bec-meltano/
# Use Meltano commands directly for testing
meltano run supabase-to-bigquery
```

## âš™ï¸ Configuration

### Environment Variables (bec-meltano/.env)

```bash
# Supabase PostgreSQL Configuration
TAP_POSTGRES_PASSWORD=your_supabase_password

# BigQuery Configuration
BQ_PROJECT_ID=dsai-468212
TARGET_STAGING_DATASET=olist_data_staging
TARGET_RAW_DATASET=olist_data_raw

# Google Cloud Service Account Key Path
GOOGLE_APPLICATION_CREDENTIALS=../service-account-key.json
```

### dbt Configuration

- **Project**: `bec_dbt/`
- **Profile**: `bec_dbt` (defined in `profiles.yml`)
- **Service Account Key**: `service-account-key.json` (in project root, path set via `GOOGLE_APPLICATION_CREDENTIALS`)
- **Target**: `dev` (BigQuery)
- **Datasets**: `olist_data_staging`, `olist_data_warehouse`

## ğŸ”§ Development Notes

### Python Version Compatibility

- **Python 3.11**: âœ… Fully supported (recommended)
- **Python 3.12**: âš ï¸ Limited support (some dependency issues)
- **Python 3.13+**: âŒ Not supported (major dependency conflicts)

### Production vs Development

- **Production**: Use Meltano ELT framework (`bec-meltano/`) for robust, containerized deployment
- **Development**: Use direct Python approach (`bec-meltano/`) for quick testing
- **Orchestration**: Dagster (`bec-dagster/`) provides workflow management and monitoring
- **Transformation**: dbt (`bec_dbt/`) provides data modeling and analytics
- **CI/CD**: Meltano provides better logging, state management, and error handling

### dbt Model Layers

The pipeline includes comprehensive dbt transformations:

- **Staging**: Raw data cleaning, deduplication, and quality flags
- **Warehouse**: Dimensional modeling with facts and dimensions
- **Analytics**: One Big Table (OBT) aggregations for business intelligence

### Troubleshooting

1. **Meltano installation issues**: Ensure Python 3.11 is active
2. **BigQuery authentication**: Check `GOOGLE_APPLICATION_CREDENTIALS` path in `bec-meltano/.env` points to your service account JSON file in project root
3. **Supabase connection**: Verify `TAP_POSTGRES_PASSWORD` in `bec-meltano/.env`
4. **dbt issues**: Check BigQuery permissions and dataset existence, verify service account key path
5. **Pipeline execution**: Check Meltano logs in `bec-meltano/.meltano/logs/`

## ğŸ“Š Pipeline Features

- âœ… **Supabase PostgreSQL data extraction with Meltano**
- âœ… **Automated BigQuery loading with multiple target configurations**
- âœ… **Production-ready Meltano ELT framework**
- âœ… **Comprehensive dbt transformations (staging â†’ warehouse â†’ analytics)**
- âœ… **Dagster orchestration with web UI monitoring**
- âœ… **Data quality checks and validation**
- âœ… **Environment-based configuration**
- âœ… **Customer segmentation and analytics macros**
- âœ… **Multiple execution modes (production/development)**

## ğŸ—‚ï¸ Key Components

### Meltano ELT Pipeline

- **`bec-meltano/meltano.yml`**: Meltano configuration with Supabase and BigQuery targets

### dbt Transformations

- **`bec_dbt/models/staging/`**: Raw data cleaning and quality flags
- **`bec_dbt/models/warehouse/`**: Dimensional modeling (dim*\*, fact*\*)
- **`bec_dbt/models/analytic/`**: One Big Table analytics aggregations
- **`bec_dbt/macros/`**: Reusable SQL macros for business logic

### Orchestration

- **`bec-dagster/dagster_pipeline.py`**: Workflow orchestration assets

## ğŸ¤ Contributing

1. Ensure Python 3.11 conda environment: `conda activate bec`
2. Install environment: `conda env create -f requirements-bec.yaml`
3. Set up credentials: `cp bec-meltano/.env.example bec-meltano/.env` and place your service account JSON file in project root, then set `GOOGLE_APPLICATION_CREDENTIALS` path in the .env file
4. Test with Meltano: `meltano run supabase-to-bigquery-with-transform`
5. Test dbt transformations: `cd bec_dbt && dbt run && dbt test`
6. Update documentation for any new features

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.
